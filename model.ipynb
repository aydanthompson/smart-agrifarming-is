{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a60b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload .py files automatically when they change, otherwise changes won't be reflected without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b8d1",
   "metadata": {},
   "source": [
    "# Smart Agrifarming - Activity Recognition Pipeline\n",
    "\n",
    "## 1. Architecture\n",
    "\n",
    "This notebook maps to the data processing and analytics stage of the proposed pipeline. It aims to convert raw, high-frequency IoT sensor streams (MPU9250/BNO055 IMUs) into actionable insights (cow activity).\n",
    "\n",
    "This notebook simulates the \"cold path\" from the architecture diagram:\n",
    "1. Loads raw CSVs, simulating data ingestion from a Data Lake.\n",
    "2. Transforms data using windowing, feature extraction, and caching.\n",
    "3. Trains two ML models.\n",
    "\n",
    "## 2. Data Ingestion\n",
    "\n",
    "The raw dataset consists of CSV files representing labelled activities (e.g., \"413_Walking_1219_20241021_104507.csv\", where \"1219\" is likely the device ID, and \"Walking\" is the activity).\n",
    "\n",
    "In a production environment with thousands of cows, re-parsing raw data (in this implementation, the raw CSVs) for every training run would be computationally expensive. To avoid this, our complete pipeline utilises a Data Warehouse. This notebook simulates the Data Warehouse component with a cache in the `.parquet` format. Parquet is a columnar storage format, allowing for significantly faster read times compared to row-based CSVs, particularly for large datasets. This caching mechanism means we can perform the initial transformation of the raw data into an optimised, analytics-ready form, which we then cache and re-use, drastically reducing computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31d0399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache already exists.\n"
     ]
    }
   ],
   "source": [
    "from smart_agrifarming.config import ACTIVITY_COL, ID_COL, TIME_COL\n",
    "from smart_agrifarming.reader import combine_datasets\n",
    "\n",
    "df_master = combine_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75f954",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Raw IMU sensor data contains acceleration, gyroscope, and magnetometer readings across multiple axes. However, machine learning models perform better when there are meaningful features that capture the patterns of different activities.\n",
    "\n",
    "Consider these two activities:\n",
    "1. Resting: Low, stable acceleration values.\n",
    "2. Walking: Rythmic, periodic acceleration patterns.\n",
    "\n",
    "Raw sensor values don't capture these distinctions, but we can extract features that can.\n",
    "\n",
    "1. Time-Domain: Captures data distribution (e.g., mean, standard deviation, range).\n",
    "2. Frequency-Domain: Patterns (e.g., walking has regular gait cycles), FFT (Fast Fourier Transform) to extract dominant frequencies.\n",
    "3. Signal Magnitude Area (SMA): Represents total movement energy.\n",
    "4. Zero-Crossing Rate: How often the signal changes direction.\n",
    "\n",
    "### Edge Computation Consideration\n",
    "\n",
    "Whilst FFT features are powerful, they are computationally expensive. The scenario's battery-constrained IoT environment means we must evaluate whether the accuracy gains justify the additional power consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f158b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features:\n",
      "   acc_mag_mean  acc_mag_std  ...  Activity                   Time\n",
      "0      9.618778     0.691991  ...   Walking  2024-05-13 14:44:00.0\n",
      "1      9.834627     0.611722  ...   Walking  2024-05-13 14:44:01.0\n",
      "2     10.125145     0.997031  ...   Walking  2024-05-13 14:44:02.0\n",
      "3      9.913394     1.185803  ...   Walking  2024-05-13 14:44:03.0\n",
      "4      9.720967     1.076403  ...   Walking  2024-05-13 14:44:04.0\n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "270428 datapoints split into 27030 windows with 80 features per window.\n"
     ]
    }
   ],
   "source": [
    "from smart_agrifarming.features import create_windowed_features\n",
    "\n",
    "# Window configuration.\n",
    "# Datapoints are every 100ms, so 20 rows is 2 seconds.\n",
    "WINDOW_SIZE = 20\n",
    "# Step 1 second at a time (50% overlap).\n",
    "STEP_SIZE = 10\n",
    "\n",
    "df_features = create_windowed_features(\n",
    "    df_master,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    step_size=STEP_SIZE,\n",
    ")\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(df_features.head())\n",
    "\n",
    "# Don't include window metadata in column calculation.\n",
    "print(f\"\\n{len(df_master)} datapoints split into {len(df_features)} windows with {len(df_features.columns) - 3} features per window.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b557ae0",
   "metadata": {},
   "source": [
    "## 4. Test-Train Split\n",
    "\n",
    "### Random Split\n",
    "\n",
    "Initially, the test-train split was achieved using scikit-learn's `test_train_split`, which randomly assigns individual datapoints into test/train sets. This caused a phenomenon called \"temporal data leakage\". Since consecutive IMU datapoints are similar, randomly distributing the datapoints between each set resulted in the model learning how to interpolate between nearby training points rather than generalising patterns in the data. The result was a high-accuracy (when benchmarked against the original data) model with poor real-world performance.\n",
    "\n",
    "### Grouped Split\n",
    "\n",
    "In order to fix the data leakage caused by randomly splitting the data, a group splitting method was introduced. The `GroupShuffleSplit`, grouped by device (using device ID), ensured all data from a device stayed in either the training or the testing set, preventing leakage. However, this introduced a new issue: class distribution. Not all devices (effectively cows) exhibited the same distribution of activities. This could be because some cows engaged in some activities more than others, but more likely, given this dataset was created and shared alongside a ML paper, the datapoints included in this dataset were specifically chosen due to their ability to best represent the patterns of each activity, and the class distribution was not an important factor when selecting the datapoints.\n",
    "\n",
    "### Manually Grouped Split\n",
    "\n",
    "Due to the low number of devices (10), manual analysis of the class distribution allowed a manually grouped split. This ensures that the data leakage problem remains solved, but keeps the class distribution between testing and training sets as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d756d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = [col for col in df_features.columns if col not in [TIME_COL, ACTIVITY_COL, ID_COL]]\n",
    "\n",
    "x = df_features[features_cols]\n",
    "y = df_features[ACTIVITY_COL]\n",
    "groups = df_features[ID_COL]\n",
    "\n",
    "# Split grouped by device.\n",
    "test_devices = [\"1217\", \"4821\", \"3120\"]\n",
    "train_devices = [\"1219\", \"1319\", \"2016\", \"3321\", \"4119\", \"6019\", \"6319\"]\n",
    "train_mask = groups.isin(train_devices)\n",
    "test_mask = groups.isin(test_devices)\n",
    "x_train = x[train_mask]\n",
    "y_train = y[train_mask]\n",
    "x_test = x[test_mask]\n",
    "y_test = y[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dcf44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features.\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0961a",
   "metadata": {},
   "source": [
    "## Fixing Class Imbalance\n",
    "\n",
    "The cow IMU dataset suffers with class imbalance. There are some activities (e.g., resting and grazing) that occur much more frequently than others (e.g., shaking and scratching). This causes the model to ignore the rarer classes (activities), as if the model just predicts the most common activity for everything, it achieves a high accuracy but learns nothing useful.\n",
    "\n",
    "To solve this, we can create synthetic samples for the rarer classes using SMOTE (Synthetic Minority Over-sampling Technique). This works by interpolating between existing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c057226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing (19578 total samples):\n",
      "Activity\n",
      "Resting       8368\n",
      "Grazing       5672\n",
      "Standing      2931\n",
      "Walking       2330\n",
      "Licking         85\n",
      "Drinking        47\n",
      "Shaking         42\n",
      "Scratching      33\n",
      "Rising          24\n",
      "Pitching        21\n",
      "Trotting        15\n",
      "LyingDown       10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after balancing (33472 total samples):\n",
      "Activity\n",
      "Walking     8368\n",
      "Standing    8368\n",
      "Grazing     8368\n",
      "Resting     8368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "13894 synthetic samples created.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Class distribution before balancing ({len(y_train)} total samples):\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Remove classes with too few samples (else real to synthetic ratio will be awful).\n",
    "MIN_SAMPLE_THRESHOLD = 100\n",
    "class_counts = y_train.value_counts()\n",
    "valid_classes = class_counts[class_counts >= MIN_SAMPLE_THRESHOLD].index\n",
    "train_mask = y_train.isin(valid_classes)\n",
    "test_mask = y_test.isin(valid_classes)\n",
    "x_train_filtered = x_train_scaled[train_mask]\n",
    "y_train_filtered = y_train[train_mask]\n",
    "x_test_filtered = x_test_scaled[test_mask]\n",
    "y_test_filtered = y_test[test_mask]\n",
    "\n",
    "# Apply SMOTE.\n",
    "min_samples = pd.Series(y_train_filtered).value_counts().min()\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42, k_neighbors=min_samples - 1)\n",
    "x_train_balanced, y_train_balanced = smote.fit_resample(x_train_filtered, y_train_filtered)\n",
    "\n",
    "print(f\"\\nClass distribution after balancing ({len(y_train_balanced)} total samples):\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "\n",
    "print(f\"\\n{len(y_train_balanced) - len(y_train)} synthetic samples created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b513c",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b826b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.8%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Grazing       0.94      0.98      0.96      2660\n",
      "     Resting       0.80      0.92      0.86      2962\n",
      "    Standing       0.32      0.06      0.10       980\n",
      "     Walking       0.62      0.83      0.71       780\n",
      "\n",
      "    accuracy                           0.82      7382\n",
      "   macro avg       0.67      0.70      0.66      7382\n",
      "weighted avg       0.77      0.82      0.78      7382\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2597    6    4   53]\n",
      " [  19 2734  118   91]\n",
      " [   4  662   59  255]\n",
      " [ 129    1    1  649]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train.\n",
    "rfc_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rfc_model.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate.\n",
    "rfc_y_pred = rfc_model.predict(x_test_filtered)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filtered, rfc_y_pred):.1%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_filtered, rfc_y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_filtered, rfc_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9256c5",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e727b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.2%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Grazing       0.96      0.96      0.96      2660\n",
      "     Resting       0.82      0.95      0.88      2962\n",
      "    Standing       0.66      0.13      0.21       980\n",
      "     Walking       0.64      0.89      0.74       780\n",
      "\n",
      "    accuracy                           0.84      7382\n",
      "   macro avg       0.77      0.73      0.70      7382\n",
      "weighted avg       0.83      0.84      0.81      7382\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2565    8   24   63]\n",
      " [  18 2825   37   82]\n",
      " [   7  598  125  250]\n",
      " [  77    2    4  697]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train.\n",
    "svc_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "svc_model.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate.\n",
    "svc_y_pred = svc_model.predict(x_test_filtered)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filtered, svc_y_pred):.1%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_filtered, svc_y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_filtered, svc_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
