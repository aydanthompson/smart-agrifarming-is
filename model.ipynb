{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a60b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload .py files automatically when they change, otherwise changes won't be reflected without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b8d1",
   "metadata": {},
   "source": [
    "# Smart Agrifarming - Activity Recognition Pipeline\n",
    "\n",
    "## 1. Architecture\n",
    "\n",
    "This notebook maps to the data processing and analytics stage of the proposed pipeline. It aims to convert raw, high-frequency IoT sensor streams (MPU9250/BNO055 IMUs) into actionable insights (cow activity).\n",
    "\n",
    "This notebook simulates the \"cold path\" from the architecture diagram:\n",
    "1. Loads raw CSVs, simulating data ingestion from a Data Lake.\n",
    "2. Transforms data using windowing, feature extraction, and caching.\n",
    "3. Trains two ML models.\n",
    "\n",
    "## 2. Data Ingestion\n",
    "\n",
    "The raw dataset consists of CSV files representing labelled activities (e.g., \"413_Walking_1219_20241021_104507.csv\", where \"1219\" is likely the device ID, and \"Walking\" is the activity).\n",
    "\n",
    "In a production environment with thousands of cows, re-parsing raw data (in this implementation, the raw CSVs) for every training run would be computationally expensive. To avoid this, our complete pipeline utilises a Data Warehouse. This notebook simulates the Data Warehouse component with a cache in the `.parquet` format. Parquet is a columnar storage format, allowing for significantly faster read times compared to row-based CSVs, particularly for large datasets. This caching mechanism means we can perform the initial transformation of the raw data into an optimised, analytics-ready form, which we then cache and re-use, drastically reducing computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31d0399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache already exists.\n"
     ]
    }
   ],
   "source": [
    "from smart_agrifarming.config import ACTIVITY_COL, ID_COL, TIME_COL\n",
    "from smart_agrifarming.reader import combine_datasets\n",
    "\n",
    "df_master = combine_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75f954",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Raw IMU sensor data contains acceleration, gyroscope, and magnetometer readings across multiple axes. However, machine learning models perform better when there are meaningful features that capture the patterns of different activities.\n",
    "\n",
    "Consider these two activities:\n",
    "1. Resting: Low, stable acceleration values.\n",
    "2. Walking: Rythmic, periodic acceleration patterns.\n",
    "\n",
    "Raw sensor values don't capture these distinctions, but we can extract features that can.\n",
    "\n",
    "1. Time-Domain: Captures data distribution (e.g., mean, standard deviation, range).\n",
    "2. Frequency-Domain: Patterns (e.g., walking has regular gait cycles), FFT (Fast Fourier Transform) to extract dominant frequencies.\n",
    "3. Signal Magnitude Area (SMA): Represents total movement energy.\n",
    "4. Zero-Crossing Rate: How often the signal changes direction.\n",
    "\n",
    "### Edge Computation Consideration\n",
    "\n",
    "Whilst FFT features are powerful, they are computationally expensive. The scenario's battery-constrained IoT environment means we must evaluate whether the accuracy gains justify the additional power consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f158b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features:\n",
      "   acc_mag_mean  acc_mag_std  acc_mag_min  acc_mag_max  acc_mag_range  \\\n",
      "0      9.618778     0.691991     7.594770    11.023458       3.428688   \n",
      "1      9.834627     0.611722     8.641084    10.983500       2.342417   \n",
      "2     10.125145     0.997031     8.641084    12.374877       3.733794   \n",
      "3      9.913394     1.185803     8.299503    12.374877       4.075374   \n",
      "4      9.720967     1.076403     8.299503    11.668529       3.369026   \n",
      "\n",
      "   gyro_mag_mean  gyro_mag_std        sma  MPU9250_AX_mean  MPU9250_AX_std  \\\n",
      "0      32.392549      7.845672  306.66028         2.809455        0.872952   \n",
      "1      43.909822     15.629573  315.07381         2.832921        0.955901   \n",
      "2      49.482484     16.805544  320.44659         2.689741        1.316436   \n",
      "3      49.692550     15.025449  305.20457         2.605702        1.584631   \n",
      "4      46.179390     17.018356  299.35293         2.740502        1.362941   \n",
      "\n",
      "   ...  MPU9250_GZ_q25  MPU9250_GZ_q75  MPU9250_GZ_iqr  MPU9250_GZ_zcr  \\\n",
      "0  ...       -6.393435       11.154173       17.547608            0.45   \n",
      "1  ...      -19.653320        5.691528       25.344848            0.35   \n",
      "2  ...      -19.515988        7.339480       26.855468            0.20   \n",
      "3  ...       -9.994505       22.796627       32.791133            0.35   \n",
      "4  ...      -11.993412       24.765013       36.758425            0.40   \n",
      "\n",
      "   MPU9250_GZ_fft_mean  MPU9250_GZ_fft_std  MPU9250_GZ_dominant_freq  \\\n",
      "0            52.721965           25.190706                       2.0   \n",
      "1            71.751157           43.490088                       2.0   \n",
      "2            72.584197           50.988880                       1.0   \n",
      "3            73.919660           53.372449                       3.0   \n",
      "4            76.927297           62.958087                       4.0   \n",
      "\n",
      "   Device ID  Activity                   Time  \n",
      "0       1217   Walking  2024-05-13 14:44:00.0  \n",
      "1       1217   Walking  2024-05-13 14:44:01.0  \n",
      "2       1217   Walking  2024-05-13 14:44:02.0  \n",
      "3       1217   Walking  2024-05-13 14:44:03.0  \n",
      "4       1217   Walking  2024-05-13 14:44:04.0  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "270428 datapoints split into 27030 windows with 80 features per window.\n"
     ]
    }
   ],
   "source": [
    "from smart_agrifarming.features import create_windowed_features\n",
    "\n",
    "# Window configuration.\n",
    "# Datapoints are every 100ms, so 20 rows is 2 seconds.\n",
    "WINDOW_SIZE = 20\n",
    "# Step 1 second at a time (50% overlap).\n",
    "STEP_SIZE = 10\n",
    "\n",
    "df_features = create_windowed_features(\n",
    "    df_master,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    step_size=STEP_SIZE,\n",
    ")\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(df_features.head())\n",
    "\n",
    "# Don't include window metadata in column calculation.\n",
    "print(f\"\\n{len(df_master)} datapoints split into {len(df_features)} windows with {len(df_features.columns) - 3} features per window.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b557ae0",
   "metadata": {},
   "source": [
    "## 4. Test-Train Split\n",
    "\n",
    "### Random Split\n",
    "\n",
    "Initially, the test-train split was achieved using scikit-learn's `test_train_split`, which randomly assigns individual datapoints into test/train sets. This caused a phenomenon called \"temporal data leakage\". Since consecutive IMU datapoints are similar, randomly distributing the datapoints between each set resulted in the model learning how to interpolate between nearby training points rather than generalising patterns in the data. The result was a high-accuracy (when benchmarked against the original data) model with poor real-world performance.\n",
    "\n",
    "### Grouped Split\n",
    "\n",
    "In order to fix the data leakage caused by randomly splitting the data, a group splitting method was introduced. The `GroupShuffleSplit`, grouped by device (using device ID), ensured all data from a device stayed in either the training or the testing set, preventing leakage. However, this introduced a new issue: class distribution. Not all devices (effectively cows) exhibited the same distribution of activities. This could be because some cows engaged in some activities more than others, but more likely, given this dataset was created and shared alongside a ML paper, the datapoints included in this dataset were specifically chosen due to their ability to best represent the patterns of each activity, and the class distribution was not an important factor when selecting the datapoints.\n",
    "\n",
    "### Manually Grouped Split\n",
    "\n",
    "Due to the low number of devices (10), manual analysis of the class distribution allowed a manually grouped split. This ensures that the data leakage problem remains solved, but keeps the class distribution between testing and training sets as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d756d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = [col for col in df_features.columns if col not in [TIME_COL, ACTIVITY_COL, ID_COL]]\n",
    "\n",
    "x = df_features[features_cols]\n",
    "y = df_features[ACTIVITY_COL]\n",
    "groups = df_features[ID_COL]\n",
    "\n",
    "# Split grouped by device.\n",
    "test_devices = [\"1217\", \"4821\", \"3120\"]\n",
    "train_devices = [\"1219\", \"1319\", \"2016\", \"3321\", \"4119\", \"6019\", \"6319\"]\n",
    "train_mask = groups.isin(train_devices)\n",
    "test_mask = groups.isin(test_devices)\n",
    "x_train = x[train_mask]\n",
    "y_train = y[train_mask]\n",
    "x_test = x[test_mask]\n",
    "y_test = y[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06739805",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling\n",
    "\n",
    "Machine learning algorithms that rely on distance calculations (like SVC) are sensitive to the magnitude of their inputs. Without scaling, features with larger ranges dominate a model's decisions, even if lower magnitude features are better predictors. In the data, features like acceleration magnitude (5-15m/s<sup>2</sup>) would dominate features like the acceleration's standard deviation (0.5-2m/s<sup>2</sup>) by 10x.\n",
    "\n",
    "The `StandardScaler` normalises each feature, ensuring all features contribute equally to the model.\n",
    "\n",
    "### Scaling Test Data\n",
    "\n",
    "To prevent the training data leaking to the testing data, the scaler is fit using solely the training data. The resultant transformation is then applied to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features.\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0961a",
   "metadata": {},
   "source": [
    "## 6. Addressing Class Imbalance\n",
    "\n",
    "The cow IMU dataset suffers with class imbalance. There are some activities (e.g., resting and grazing) that occur much more frequently than others (e.g., shaking and scratching). Without addressing this imbalance, the model optimises for accuracy by predicting classes that occur more frequently, whilst completely ignoring rare classes even if the underlying features are more closely aligned with them.\n",
    "\n",
    "### SMOTE\n",
    "\n",
    "To solve this, we can create synthetic samples for the rarer classes using SMOTE (Synthetic Minority Over-sampling Technique), which works by interpolating between existing samples.\n",
    "\n",
    "Whilst this ensures the model trains on all classes equally, synthetic data is no substitute for real data. The reality of these rarer classes may be more varied than the existing data implies, something that interpolated data cannot replicate. This problem is particularly prominent in classes with very low sample sizes, hence classes with less than 100 samples have been completely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c057226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing (19578 total samples):\n",
      "Activity\n",
      "Resting       8368\n",
      "Grazing       5672\n",
      "Standing      2931\n",
      "Walking       2330\n",
      "Licking         85\n",
      "Drinking        47\n",
      "Shaking         42\n",
      "Scratching      33\n",
      "Rising          24\n",
      "Pitching        21\n",
      "Trotting        15\n",
      "LyingDown       10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after balancing (33472 total samples):\n",
      "Activity\n",
      "Walking     8368\n",
      "Standing    8368\n",
      "Grazing     8368\n",
      "Resting     8368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "13894 synthetic samples created.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Class distribution before balancing ({len(y_train)} total samples):\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Remove classes with too few samples (else real to synthetic ratio will be awful).\n",
    "MIN_SAMPLE_THRESHOLD = 100\n",
    "class_counts = y_train.value_counts()\n",
    "valid_classes = class_counts[class_counts >= MIN_SAMPLE_THRESHOLD].index\n",
    "train_mask = y_train.isin(valid_classes)\n",
    "test_mask = y_test.isin(valid_classes)\n",
    "x_train_filtered = x_train_scaled[train_mask]\n",
    "y_train_filtered = y_train[train_mask]\n",
    "x_test_filtered = x_test_scaled[test_mask]\n",
    "y_test_filtered = y_test[test_mask]\n",
    "\n",
    "# Apply SMOTE.\n",
    "min_samples = pd.Series(y_train_filtered).value_counts().min()\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42, k_neighbors=min_samples - 1)\n",
    "x_train_balanced, y_train_balanced = smote.fit_resample(x_train_filtered, y_train_filtered)\n",
    "\n",
    "print(f\"\\nClass distribution after balancing ({len(y_train_balanced)} total samples):\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "\n",
    "print(f\"\\n{len(y_train_balanced) - len(y_train)} synthetic samples created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b513c",
   "metadata": {},
   "source": [
    "## 7. Models\n",
    "\n",
    "### Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b826b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.8%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Grazing       0.94      0.98      0.96      2660\n",
      "     Resting       0.80      0.92      0.86      2962\n",
      "    Standing       0.32      0.06      0.10       980\n",
      "     Walking       0.62      0.83      0.71       780\n",
      "\n",
      "    accuracy                           0.82      7382\n",
      "   macro avg       0.67      0.70      0.66      7382\n",
      "weighted avg       0.77      0.82      0.78      7382\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2597    6    4   53]\n",
      " [  19 2734  118   91]\n",
      " [   4  662   59  255]\n",
      " [ 129    1    1  649]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train.\n",
    "rfc_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rfc_model.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate.\n",
    "rfc_y_pred = rfc_model.predict(x_test_filtered)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filtered, rfc_y_pred):.1%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_filtered, rfc_y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_filtered, rfc_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9256c5",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e727b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.2%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Grazing       0.96      0.96      0.96      2660\n",
      "     Resting       0.82      0.95      0.88      2962\n",
      "    Standing       0.66      0.13      0.21       980\n",
      "     Walking       0.64      0.89      0.74       780\n",
      "\n",
      "    accuracy                           0.84      7382\n",
      "   macro avg       0.77      0.73      0.70      7382\n",
      "weighted avg       0.83      0.84      0.81      7382\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2565    8   24   63]\n",
      " [  18 2825   37   82]\n",
      " [   7  598  125  250]\n",
      " [  77    2    4  697]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train.\n",
    "svc_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "svc_model.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate.\n",
    "svc_y_pred = svc_model.predict(x_test_filtered)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filtered, svc_y_pred):.1%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_filtered, svc_y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_filtered, svc_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
